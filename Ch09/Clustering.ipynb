{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering time series for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import homogeneity_score, completeness_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/50words_TEST.csv',\n",
    "                   header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.rename(columns = {0:'word'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(words.iloc[1, 1:-1])\n",
    "plt.title(\"Sample Projection Word \" + str(words.word[1]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.hist(words.iloc[1, 1:-1], 10)\n",
    "plt.title(\"Histogram of Projection Word \" + str(words.word[1]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(words.iloc[3, 1:-1])\n",
    "plt.title(\"Sample Projection Word \" + str(words.word[3]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.hist(words.iloc[3, 1:-1], 10)\n",
    "plt.title(\"Histogram of Projection Word \" + str(words.word[3]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(words.iloc[5, 1:-1])\n",
    "plt.title(\"Sample Projection Word \" + str(words.word[11]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.hist(words.iloc[5, 1:-1], 10)\n",
    "plt.title(\"Histogram of Projection Word \" + str(words.word[11]), fontweight = 'bold', y = 0.8, fontsize = 14)\n",
    "plt.suptitle(\"Sample word projections and histograms of the projections\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## We can also consider the 2d histogram of a word\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "w = 23\n",
    "selected_words = words[words.word == w]\n",
    "selected_words.shape\n",
    "\n",
    "for idx, row in selected_words.iterrows():\n",
    "    y = np.hstack([y, row[1:271]])\n",
    "    x = np.hstack([x, np.array(range(270))])\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "hist = ax.hist2d(x, y, bins = 50)\n",
    "plt.xlabel(\"Time\", fontsize = 18)\n",
    "plt.ylabel(\"Value\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_features = words.iloc[:, 1:271]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some features from original time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times  = []\n",
    "values = []\n",
    "for idx, row in words_features.iterrows():\n",
    "    values.append(row.values)\n",
    "    times.append(np.array([i for i in range(row.values.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cesium import featurize\n",
    "# features_to_use = [\"amplitude\",\n",
    "#                    \"percent_beyond_1_std\",\n",
    "#                    \"percent_close_to_median\",\n",
    "#                    ]\n",
    "# featurized_words = featurize.featurize_time_series(times=times,\n",
    "#                                               values=values,\n",
    "#                                               errors=None,\n",
    "#                                               features_to_use=features_to_use,\n",
    "#                                               scheduler = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_words = pd.read_csv(\"data/featurized_words.csv\", header = [0, 1])\n",
    "featurized_words.columns = featurized_words.columns.droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(featurized_words.percent_beyond_1_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some features from histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times = []\n",
    "# values = []\n",
    "# for idx, row in words_features.iterrows():\n",
    "#     values.append(np.histogram(row.values, bins=10, range=(-2.5, 5.0))[0] + .0001) ## cesium seems not to handle 0s\n",
    "#     times.append(np.array([i for i in range(9)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_use = [\"amplitude\",\n",
    "#                    \"percent_close_to_median\",\n",
    "#                   \"skew\"\n",
    "#                   ]\n",
    "# featurized_hists = featurize.featurize_time_series(times=times,\n",
    "#                                               values=values,\n",
    "#                                               errors=None,\n",
    "#                                               features_to_use=features_to_use,\n",
    "#                                               scheduler = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurized_hists.to_csv(\"data/featurized_hists.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_hists = pd.read_csv(\"data/featurized_hists.csv\", header = [0, 1])\n",
    "featurized_hists.columns = featurized_hists.columns.droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_hists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([featurized_words.reset_index(drop=True), featurized_hists], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we also add some of our own features again, to account more for shape\n",
    "feats = np.zeros( (words.shape[0], 1), dtype = np.float32)\n",
    "for i in range(words.shape[0]):\n",
    "    vals = words.iloc[i, 1:271].values\n",
    "    feats[i, 0] = np.where(vals == np.max(vals))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['peak_location'] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values = preprocessing.scale(features.iloc[:, [1, 2, 3, 5, 6, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=50, linkage='ward')\n",
    "clustering.fit(feature_values)\n",
    "words['feature_label'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['feature_label'] = words.feature_label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the number of feature labels \n",
    "results = words.groupby('word')['feature_label'].agg({'num_clustering_labels': lambda x: len(set(x)),\n",
    "                                            'num_word_samples':      lambda x: len(x),\n",
    "                                            'most_common_label':     lambda x: Counter(x).most_common(1)[0][0]})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the number of feature labels \n",
    "results_feats = words.groupby('feature_label')['word'].agg({'num_words': lambda x: len(set(x)),\n",
    "                                            'num_feat_samples':      lambda x: len(x),\n",
    "                                            'most_common_word':     lambda x: Counter(x).most_common(1)[0][0]})\n",
    "results_feats\n",
    "## note that word 1 = most common in cluster 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score(words.word, words.feature_label)\n",
    "## see definitions in user manual: https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping Distance Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.sin(np.linspace(1, 10))\n",
    "ts2 = np.sin(2 * np.linspace(1, 10))\n",
    "ts3 = np.zeros((50,)) \n",
    "plt.plot(ts1)\n",
    "plt.plot(ts2)\n",
    "plt.plot(ts3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: calculate the Euclidean distance between respective pairs of time series from the 3 time series above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(ts1 - ts2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(ts1 - ts3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(ts2 - ts3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1,10).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another time series clustering technique that has been recommended is a correlation measure. How does this fair in the case of our sine curves and straigh line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(215202)\n",
    "ts3_noise = np.random.random(ts3.shape)\n",
    "ts3 = np.zeros((50,)) \n",
    "ts3 = ts3 + ts3_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ts1, ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ts1, ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ts2, ts3 + np.random.random(ts3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: use what we discussed about dynamic programming to code a DTW function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = words.iloc[:, 1:271].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distDTW(ts1, ts2):\n",
    "    DTW       = np.full((len(ts1) + 1, len(ts2) + 1), 0, dtype = np.float32)\n",
    "    DTW[:, 0] = np.inf\n",
    "    DTW[0, :] = np.inf\n",
    "    DTW[0, 0] = 0\n",
    "\n",
    "    for i in range(1, len(ts1) + 1):\n",
    "        for j in range(1, len(ts2) + 1):\n",
    "            idx1 = i - 1 \n",
    "            idx2 = j - 1\n",
    "            \n",
    "            dist               = (ts1[idx1] - ts2[idx2])**2\n",
    "            min_preceding_dist = min(DTW[i-1, j],DTW[i, j-1], DTW[i-1, j-1])\n",
    "\n",
    "            DTW[i, j] = dist + min_preceding_dist\n",
    "\n",
    "    return sqrt(DTW[len(ts1), len(ts2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: does this fix the problem above noted with the sine curves vs. a straight line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDTW(ts1, ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDTW(ts1, ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDTW(ts2, ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDTW(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw.distance(X[0], X[1])\n",
    "## worth checking out: https://github.com/wannesm/dtaidistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pairwise_distances(X, metric = distDTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"pairwise_word_distances.npy\", \"wb\") as f:\n",
    "#     np.save(f, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.load(\"data/pairwise_word_distances.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try clustering based on dynamic time warping distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use hierarchical clustering as a distance agnostic methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='average', n_clusters=50, affinity = 'precomputed') \n",
    "## 'average' linkage is good for non Euclidean distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clustering.fit_predict(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: How did the clustering perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(homogeneity_score(words.word, labels))\n",
    "print(completeness_score(words.word, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quoting: https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness\n",
    "# homogeneity: each cluster contains only members of a single class.\n",
    "# completeness: all members of a given class are assigned to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = contingency_matrix(labels, words.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note difficulties in assessing this given imbalanced dataset\n",
    "plt.imshow(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
