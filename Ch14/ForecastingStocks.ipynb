{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "width = 6\n",
    "height = 3\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [width, height]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:  Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we download historical data from 1990-2019\n",
    "## from Yahoo https://finance.yahoo.com/quote/%5EGSPC/history?period1=634885200&period2=1550034000&interval=1d&filter=history&frequency=1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sp500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's first take a look at our data\n",
    "df.index = df.Date\n",
    "fig = df.Close.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.Close - df.Open).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can see there have been several \"regime changes\"\n",
    "## although it would be difficult to set an exact date of the change\n",
    "## but do different weeks look all that different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = df[\"1990-05-05\":\"1990-05-11\"].Close.values\n",
    "mean_val = np.mean(vals)\n",
    "plt.plot([1, 2, 3, 4, 5], vals/mean_val)\n",
    "plt.xticks([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = df[\"2000-05-05\":\"2000-05-11\"].Close.values\n",
    "mean_val = np.mean(vals)\n",
    "plt.plot([1, 2, 3, 4, 5], vals/mean_val)\n",
    "plt.xticks([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vals = df[\"2010-05-05\":\"2010-05-12\"].Close.values\n",
    "mean_val = np.mean(vals)\n",
    "plt.plot(vals/mean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = df[\"2018-05-05\":\"2018-05-11\"].Close.values\n",
    "mean_val = np.mean(vals)\n",
    "plt.plot([1, 2, 3, 4, 5], vals/mean_val)\n",
    "plt.xticks([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if we evaluate in terms of percent change within the week \n",
    "## none of these weeks seem distinctly different at the week-based scale to the eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use a deep learning approach, so we need to normalize our inputs to fall \n",
    "## within -1 to 1. we want to do so without letting information leak backwards from the future\n",
    "## so we need to have a rolling smoothing process rather than taking the global mean to normalize\n",
    "## these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we want to predict daily returns (imagine you choose only to buy at start of day \n",
    "## and sell at end of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Return'] = df.Close - df.Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Return.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DailyVolatility'] = df.High - df.Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DailyVolatility.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as our inputs we will use daily volatility, daily return, and daily volume\n",
    "## all should be scaled appropriately so we need to compute rolling means to scale these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will use an exponentially weighted moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewdf = df.ewm(halflife = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewdf.DailyVolatility.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vewdf = df.ewm(halflife = 10).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notice that we don't fit to the smoothed values we merely use them to \n",
    "((df.DailyVolatility - ewdf.DailyVolatility)/ vewdf.DailyVolatility**0.5 ).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScaledVolatility'] = ((df.DailyVolatility - ewdf.DailyVolatility)/ vewdf.DailyVolatility**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScaledReturn'] = ((df.Return - ewdf.Return)/ vewdf.Return**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScaledVolume'] = ((df.Volume - ewdf.Volume)/ vewdf.Volume**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove first row, which has na\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we need to form input arrays and target arrays\n",
    "## let's try to predict just a day ahead and see how we do\n",
    "## predicting stock prices is notoriously difficult so we should not\n",
    "## get ahead of ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:7000]\n",
    "test_df = df[7000:]\n",
    "X = train_df[:(7000 - 10)][[\"ScaledVolatility\", \"ScaledReturn\", \"ScaledVolume\"]].values\n",
    "Y = train_df[10:][\"ScaledReturn\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## however batches are usually in form TNC\n",
    "## time, num examples, channels\n",
    "## so we need to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: reshape X into 'TNC' form with numpy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.split(X, X.shape[0]/10, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(X, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:(7000 - 10)][[\"ScaledReturn\"]].values[:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notice that we only used each data point once\n",
    "## but actually each data point can belong to many series, occupying a different position in the series\n",
    "## say it could be the first point or the last point or a middle point in the time series\n",
    "## rather than explicitly expanding out, we will simply cut off a random number of points\n",
    "## at each end so that for each epoch through training, we'll have different series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN    = 8\n",
    "NUM_LAYERS    = 1\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS        = 10\n",
    "BATCH_SIZE    = 64\n",
    "WINDOW_SIZE   = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinp = tf.placeholder(dtype = tf.float32, shape = [WINDOW_SIZE, None, 3])\n",
    "Yinp = tf.placeholder(dtype = tf.float32, shape = [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"scope1\", reuse=tf.AUTO_REUSE):\n",
    "    #rnn_cell = tf.nn.rnn_cell.LSTMCell(NUM_HIDDEN, dtype = tf.float32)\n",
    "    #rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, output_keep_prob=0.9)\n",
    "    #rnn_output, states = tf.nn.dynamic_rnn(rnn_cell, Xinp, dtype=tf.float32) \n",
    "    \n",
    "    ## tf.nn.rnn_cell.MultiRNNCell\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(num_units=NUM_HIDDEN) for n in range(NUM_LAYERS)]\n",
    "    stacked_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    rnn_output, states = tf.nn.dynamic_rnn(stacked_rnn_cell, Xinp, dtype=tf.float32) \n",
    "    W = tf.get_variable(\"W_fc\", [NUM_HIDDEN, 1], initializer = tf.random_uniform_initializer(-.2, .2))\n",
    "    output = tf.squeeze(tf.matmul(rnn_output[-1, :, :], W))\n",
    "    ## notice we have no bias because we expect average zero return\n",
    "    loss = tf.nn.l2_loss(output - Yinp)\n",
    "    opt = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "    ##opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    train_step = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to loop through data and find a way to jitter data \n",
    "## then need to also compute validation loss\n",
    "## and need to record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each epoch\n",
    "y_hat_dict = {}\n",
    "Y_dict = {}\n",
    "\n",
    "in_sample_Y_dict = {}\n",
    "in_sample_y_hat_dict = {}\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    ## for each offset to create a new series of distinct time series \n",
    "    ## (re: overlapping issue we talked about previously)\n",
    "    epoch_training_loss = 0.0\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        X = train_df[:(7000 - WINDOW_SIZE)][[\"ScaledVolatility\", \"ScaledReturn\", \"ScaledVolume\"]].values\n",
    "        Y = train_df[WINDOW_SIZE:][\"ScaledReturn\"].values\n",
    "\n",
    "        ## make it divisible by window size\n",
    "        num_to_unpack = math.floor(X.shape[0] / WINDOW_SIZE)\n",
    "        start_idx = X.shape[0] - num_to_unpack * WINDOW_SIZE\n",
    "        X = X[start_idx:] \n",
    "        Y = Y[start_idx:]  \n",
    "        \n",
    "        X = X[i:-(WINDOW_SIZE-i)]\n",
    "        Y = Y[i:-(WINDOW_SIZE-i)]                                \n",
    "        \n",
    "        X = np.expand_dims(X, axis = 1)\n",
    "        X = np.split(X, X.shape[0]/WINDOW_SIZE, axis = 0)\n",
    "        X = np.concatenate(X, axis = 1)\n",
    "        Y = Y[::WINDOW_SIZE]\n",
    "        ## TRAINING\n",
    "        ## now batch it and run a sess\n",
    "        for j in range(math.ceil(Y.shape[0] / BATCH_SIZE)):\n",
    "            ll = BATCH_SIZE * j\n",
    "            ul = BATCH_SIZE * (j + 1)\n",
    "            \n",
    "            if ul > X.shape[1]:\n",
    "                ul = X.shape[1] - 1\n",
    "                ll = X.shape[1]- BATCH_SIZE\n",
    "            \n",
    "            training_loss, _, y_hat = sess.run([loss, train_step, output],\n",
    "                                       feed_dict = {\n",
    "                                           Xinp: X[:, ll:ul, :], Yinp: Y[ll:ul]\n",
    "                                       })\n",
    "            epoch_training_loss += training_loss          \n",
    "            \n",
    "            in_sample_Y_dict[ep]     = Y[ll:ul] ## notice this will only net us the last part of data trained on\n",
    "            in_sample_y_hat_dict[ep] = y_hat\n",
    "            \n",
    "        ## TESTING\n",
    "        X = test_df[:(test_df.shape[0] - WINDOW_SIZE)][[\"ScaledVolatility\", \"ScaledReturn\", \"ScaledVolume\"]].values\n",
    "        Y = test_df[WINDOW_SIZE:][\"ScaledReturn\"].values\n",
    "        num_to_unpack = math.floor(X.shape[0] / WINDOW_SIZE)\n",
    "        start_idx = X.shape[0] - num_to_unpack * WINDOW_SIZE\n",
    "        X = X[start_idx:] ## better to throw away beginning than end of training period when must delete\n",
    "        Y = Y[start_idx:]                              \n",
    "        \n",
    "        X = np.expand_dims(X, axis = 1)\n",
    "        X = np.split(X, X.shape[0]/WINDOW_SIZE, axis = 0)\n",
    "        X = np.concatenate(X, axis = 1)\n",
    "        Y = Y[::WINDOW_SIZE]\n",
    "        testing_loss, y_hat = sess.run([loss, output],\n",
    "                                 feed_dict = { Xinp: X, Yinp: Y })\n",
    "        ## nb this is not great. we should really have a validation loss apart from testing\n",
    "        \n",
    "    print(\"Epoch: %d   Training loss: %0.2f   Testing loss %0.2f:\" % (ep, epoch_training_loss, testing_loss))\n",
    "    Y_dict[ep] = Y\n",
    "    y_hat_dict[ep] = y_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_dict[EPOCHS - 1])\n",
    "plt.plot(y_hat_dict[EPOCHS - 1], 'r')\n",
    "plt.title('Out of sample performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(in_sample_Y_dict[EPOCHS - 1])\n",
    "plt.plot(in_sample_y_hat_dict[EPOCHS - 1], 'r')\n",
    "plt.title('In sample performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(Y_dict[EPOCHS - 1], y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(in_sample_Y_dict[EPOCHS - 1], in_sample_y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(Y_dict[EPOCHS - 1], y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(in_sample_Y_dict[EPOCHS - 1], in_sample_y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_dict[EPOCHS - 1])\n",
    "plt.plot(y_hat_dict[EPOCHS - 1] * 10, 'r')\n",
    "plt.title('Rescaled out of sample performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(in_sample_Y_dict[EPOCHS - 1])\n",
    "plt.plot(in_sample_y_hat_dict[EPOCHS - 1] * 10, 'r')\n",
    "plt.title('Rescaled in sample performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_dict[EPOCHS - 1], y_hat_dict[EPOCHS - 1] * 10, linestyle=\"\", marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(Y_dict[EPOCHS - 1], y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(Y_dict[EPOCHS - 1], y_hat_dict[EPOCHS - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_Y_dict[EPOCHS-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
